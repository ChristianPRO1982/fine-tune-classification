{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from labels import LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/teamspace/studios/this_studio/max/dataset_resume.csv\"\n",
    "#data = \"./dataset_resume.csv\"\n",
    "df = pd.read_csv(data, usecols=LABELS + ['description', 'resume'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def setup(df_orig: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_orig.copy()\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # s'il y a un résumé, on l'utilise comme description\n",
    "        df.loc[index, \"description\"] = (\n",
    "                row.resume\n",
    "                if isinstance(row.resume, str)\n",
    "                else row.description\n",
    "            )\n",
    "\n",
    "    df['label'] = df[LABELS].values.tolist()\n",
    "    df = df.rename(columns={'description': 'text'})\n",
    "    label = [list_label for list_label in df.label]\n",
    "    return df.text.tolist(), label#.astype('float32')\n",
    "\n",
    "features = df[LABELS]\n",
    "\n",
    "texts, labels = setup(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CamembertTokenizer, AutoTokenizer\n",
    "from transformers import CamembertForSequenceClassification, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at almanach/camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"almanach/camembert-base\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained(checkpoint)\n",
    "model = CamembertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels[0]), problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=324):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        encoding = self.tokenizer(text, \n",
    "                                  padding=\"max_length\", \n",
    "                                  max_length=self.max_len,\n",
    "                                  truncation=True,\n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EventDataset(train_texts, train_labels, tokenizer)\n",
    "val_ds = EventDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Label Classification Evaluation Metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "def multi_labels_metrics(predictions, labels, threshold=0.3):\n",
    "  sigmoid = torch.nn.Sigmoid()\n",
    "  probs = sigmoid(torch.Tensor(predictions))\n",
    "\n",
    "  y_pred = np.zeros(probs.shape)\n",
    "  y_pred[np.where(probs>=threshold)] = 1\n",
    "  y_true = labels\n",
    "\n",
    "  f1 = f1_score(y_true, y_pred, average = 'macro')\n",
    "  roc_auc = roc_auc_score(y_true, y_pred, average = 'macro')\n",
    "  hamming = hamming_loss(y_true, y_pred)\n",
    "\n",
    "  metrics = {\n",
    "      \"roc_auc\": roc_auc,\n",
    "      \"hamming_loss\": hamming,\n",
    "      \"f1\": f1\n",
    "  }\n",
    "\n",
    "  return metrics\n",
    "\n",
    "def compute_metrics(p:EvalPrediction):\n",
    "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "\n",
    "  result = multi_labels_metrics(predictions=preds,\n",
    "                                labels=p.label_ids)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=300,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    eval_steps=10,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=train_ds,\n",
    "                  eval_dataset = val_ds, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='3900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  91/3900 00:51 < 36:59, 1.72 it/s, Epoch 7/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.767039</td>\n",
       "      <td>0.058226</td>\n",
       "      <td>0.569341</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>169.836000</td>\n",
       "      <td>8.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.163735</td>\n",
       "      <td>0.761228</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.558799</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>169.063000</td>\n",
       "      <td>8.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.168069</td>\n",
       "      <td>0.761420</td>\n",
       "      <td>0.060363</td>\n",
       "      <td>0.557858</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>168.805000</td>\n",
       "      <td>8.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169989</td>\n",
       "      <td>0.762445</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.566533</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>169.445000</td>\n",
       "      <td>8.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.168844</td>\n",
       "      <td>0.772779</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.584103</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>168.221000</td>\n",
       "      <td>8.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.166330</td>\n",
       "      <td>0.758090</td>\n",
       "      <td>0.059295</td>\n",
       "      <td>0.555409</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>168.285000</td>\n",
       "      <td>8.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169071</td>\n",
       "      <td>0.767360</td>\n",
       "      <td>0.056090</td>\n",
       "      <td>0.575887</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>168.728000</td>\n",
       "      <td>8.653000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=91, training_loss=0.043861721898173237, metrics={'train_runtime': 52.2528, 'train_samples_per_second': 1791.29, 'train_steps_per_second': 74.637, 'total_flos': 363707751633408.0, 'train_loss': 0.043861721898173237, 'epoch': 7.0})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/4 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.163735494017601,\n",
       " 'eval_roc_auc': 0.7612276623072908,\n",
       " 'eval_hamming_loss': 0.056623931623931624,\n",
       " 'eval_f1': 0.5587993872052183,\n",
       " 'eval_runtime': 0.4631,\n",
       " 'eval_samples_per_second': 168.424,\n",
       " 'eval_steps_per_second': 8.637,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/teamspace/studios/this_studio/max/camembert-tourism-events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/teamspace/studios/this_studio/max/features.pkl\", \"wb\") as f:\n",
    "  pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Après de multiples tournées à succès et plus de 2 millions de billets vendus, Messmer, connu et reconnu comme le Maître Mondial de l’Hypnose revient près de chez vous !\n",
    "\n",
    "Dans 13Hz, Messmer vous invite à entrer dans son mystérieux et hilarant univers où la frontière entre la réalité et l’illusion s’efface, pour diriger vos pensées vers des territoires inconnus.\n",
    "\n",
    "Avec sa présence charismatique inégalée et son talent exceptionnel, le recordman en hypnose collective avec 1066 personnes hypnotisées en moins de 5 minutes, vous plonge au cœur de vos pensées les plus profondes avant de vous guider à travers un jeu subtil d’ondes cérébrales à 13Hz.\n",
    "\n",
    "Le fascinateur vous entraîne vers un état de conscience unique où la volonté et le contrôle de nos vies prennent une nouvelle dimension.\n",
    "\n",
    "Osez découvrir l’expérience Messmer, où la maîtrise de soi et la fascination se rencontrent.\"\"\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors='pt')\n",
    "encoding.to(trainer.model.device)\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Détente': 0.5166682600975037,\n",
       " 'Atelier': 0.3122008740901947,\n",
       " 'Spectacle': 0.09285923093557358,\n",
       " 'Santé': 0.08189744502305984,\n",
       " 'Concert': 0.07728264480829239,\n",
       " 'Famille': 0.07240385562181473,\n",
       " 'Danse': 0.048642776906490326,\n",
       " 'Jeu': 0.028794407844543457,\n",
       " 'Théatre': 0.017140071839094162,\n",
       " 'Sport': 0.014915966428816319,\n",
       " 'Exposition': 0.012590804137289524,\n",
       " 'Culture': 0.009760606102645397,\n",
       " 'Environnement': 0.009404394775629044,\n",
       " 'Art': 0.009255075827240944,\n",
       " 'Festival': 0.008814648725092411,\n",
       " 'Fête': 0.00797022134065628,\n",
       " 'Brocante': 0.007022770121693611,\n",
       " 'Gastronomie': 0.006755351088941097,\n",
       " 'Visite': 0.005003015510737896,\n",
       " 'Balade': 0.003963761031627655,\n",
       " 'Action': 0.0037308589089661837,\n",
       " 'Marché': 0.0035765995271503925,\n",
       " 'Conférence': 0.002429952146485448,\n",
       " 'Histoire': 0.0015247827395796776}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(outputs.logits[0].cpu()).detach().numpy()\n",
    "preds = np.zeros(probs.shape)\n",
    "##preds[np.where(probs>=0.3)] = 1\n",
    "\n",
    "\n",
    "def sort_dict_by_value(dict1):\n",
    "  \"\"\"Trie un dictionnaire par ordre décroissant des valeurs.\"\"\"\n",
    "  sorted_dict = {x: y for x, y in sorted(dict1.items(), key=lambda item: item[1], reverse=True)}\n",
    "  return sorted_dict\n",
    "  \n",
    "preds = '{'\n",
    "for idx, label in enumerate(LABELS):\n",
    "    preds += f'\"{label}\": {probs[idx]},'\n",
    "\n",
    "preds = preds.strip(',')\n",
    "preds += '}'\n",
    "preds = json.loads(preds)\n",
    "sort_dict_by_value(preds)\n",
    "#probs.\n",
    "#features.inverse_transform(preds.reshape(1,-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
