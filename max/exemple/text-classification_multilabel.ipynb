{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552ec552",
   "metadata": {},
   "source": [
    "# SetFit for Multilabel Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7f258-5aaf-47c2-b81e-2f10fc349812",
   "metadata": {},
   "source": [
    "In this notebook, we'll learn how to do few-shot text classification on a multilabel dataset with SetFit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5604f73-f395-42cb-8082-9974a87ef9e9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f09e23-2e1f-41f6-bb40-a30d447a0541",
   "metadata": {},
   "source": [
    "If you're running this Notebook on Colab or some other cloud platform, you will need to install the `setfit` library. Uncomment the following cell and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277ed9b9-459c-465e-b750-13759978e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install setfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4d3b4-93cd-4774-8055-35a00b11f483",
   "metadata": {},
   "source": [
    "To be able to share your model with the community, there are a few more steps to follow.\n",
    "\n",
    "First, you have to store your authentication token from the Hugging Face Hub (sign up [here](https://huggingface.co/join) if you haven't already!). To do so, execute the following cell and input an [access token](https://huggingface.co/docs/hub/security-tokens) associated with your account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526a3b86-db3c-4c27-bb6c-eb39d73326f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309b7d5-1736-46be-b721-eb4ee4ab9e67",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS, which you can do by uncommenting and running following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efb134e-fc40-42f2-b2a5-47112b6f2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549981d3",
   "metadata": {},
   "source": [
    "Finally, you may need to configue Git on your system by providing details about who you are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b950f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git config --global user.email \"you@example.com\"\n",
    "# !git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a8fcd-46fe-43e4-835e-57b84964358a",
   "metadata": {},
   "source": [
    "This notebook is designed to work with any multiclass [text classification dataset](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads) and pretrained [Sentence Transformer](https://huggingface.co/models?library=sentence-transformers&sort=downloads) on the Hub. Change the values below to try a different dataset / model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41542e15-d211-45e9-b428-e1532c525f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximekuil/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/maximekuil/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/datasets/load.py:1491: FutureWarning: The repository for ethos contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ethos\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "model_id = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "dataset = load_dataset(\"ethos\", \"multilabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e756be8-3b60-4c86-aa1b-7ef78289b8e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading and sampling the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b26a3-1c56-4373-8060-97f747168ede",
   "metadata": {},
   "source": [
    "Most datasets on the Hub have many more labeled examples than those one encounters in few-shot settings. To simulate the effect of training on a limited number of examples, let's subsample the training set to have at least 8 labeled examples per feature.\n",
    "\n",
    "Note that if your dataset has differently formatted labels, you may need to adapt this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb9bc83-bf99-422c-b123-4fcaed168bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['violence',\n",
       " 'directed_vs_generalized',\n",
       " 'gender',\n",
       " 'race',\n",
       " 'national_origin',\n",
       " 'disability',\n",
       " 'religion',\n",
       " 'sexual_orientation']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = dataset[\"train\"].column_names\n",
    "features.remove(\"text\")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c099cb8b-2ad6-4782-a3e6-574630956d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 8\n",
    "samples = np.concatenate(\n",
    "    [np.random.choice(np.where(dataset[\"train\"][f])[0], num_samples) for f in features]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46662ff-aa4b-49c1-9c86-c8450e6d547b",
   "metadata": {},
   "source": [
    "We encode the emotions in a single `'label'` feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9fd491-8b80-4959-adcd-630bd17f951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(record):\n",
    "    return {\"labels\": [record[feature] for feature in features]}\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e154e9-a255-4d1f-b8ff-53c46f91a142",
   "metadata": {},
   "source": [
    "Next, we use the samples we selected as our training set, and the others as our test set (since the ethos dataset does not have a test split on the hub).\n",
    "\n",
    "Here we have 64 total examples to train with since the `ethos` dataset has 8 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc7f32d-7320-4721-9276-ec37a39d2630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "text: string\n",
       "violence: int64\n",
       "directed_vs_generalized: int64\n",
       "gender: int64\n",
       "race: int64\n",
       "national_origin: int64\n",
       "disability: int64\n",
       "religion: int64\n",
       "sexual_orientation: int64\n",
       "labels: list<item: int64>\n",
       "  child 0, item: int64\n",
       "----\n",
       "text: [[\"You should know women's sports are a joke\",\"You look like Sloth with deeper Down’s syndrome\",\"You look like Russian and speak like Indian. Both are disgusting go kill yourself\",\"Women deserve to be abused, I guess.\",\"Women are made for making babies and cooking dinner and nothing else!!!\",...,\"All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles\",\"Actually THIS WEIRD PEOPLE are HARMFUL! They need medications. His mother should have know he is like this and to not deny it\",\"ABU DA BI ABA DU BA BA HAMUD! See? I’m an arab priest too. This is how they sound. right? They are jokes\",\"a homo? so you fuck your brother instead of your sister?\",\"(((They))) will be chased out of every white country and hunted down like the animals they are for their evil schemes and plots\"]]\n",
       "violence: [[0,0,1,1,0,...,0,0,0,0,0]]\n",
       "directed_vs_generalized: [[0,1,1,0,0,...,0,1,0,1,0]]\n",
       "gender: [[1,0,0,1,1,...,1,0,0,0,0]]\n",
       "race: [[0,0,0,0,0,...,0,0,0,0,1]]\n",
       "national_origin: [[0,0,1,0,0,...,0,0,0,0,0]]\n",
       "disability: [[0,1,0,0,0,...,0,1,0,0,0]]\n",
       "religion: [[0,0,0,0,0,...,0,0,1,0,0]]\n",
       "sexual_orientation: [[0,0,0,0,0,...,1,0,0,1,0]]\n",
       "labels: [[[0,0,1,0,0,0,0,0],[0,1,0,0,0,1,0,0],...,[0,1,0,0,0,0,0,1],[0,0,0,1,0,0,0,0]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"].select(samples)\n",
    "eval_dataset = dataset[\"train\"].select(\n",
    "    np.setdiff1d(np.arange(len(dataset[\"train\"])), samples)\n",
    ")\n",
    "\n",
    "train_dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bd547-0e39-43ab-adf0-5f5509217020",
   "metadata": {},
   "source": [
    "Okay, now we have the dataset, let's load and train a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7c839-1f06-4d35-aa34-6e13659db814",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8c41a",
   "metadata": {},
   "source": [
    "To train a SetFit model, the first thing to do is download a pretrained checkpoint from the Hub. We can do so by using the `from_pretrained()` method associated with the `SetFitModel` class.\n",
    "\n",
    "**Note that the `multi_target_strategy` parameter here signals to both the model and the trainer to expect a multi-labelled dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33661c9d-46d3-42eb-9b15-8a2bc49d7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximekuil/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "model = SetFitModel.from_pretrained(model_id, multi_target_strategy=\"one-vs-rest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7521e-95ca-431a-8d7f-2f18e1de16ce",
   "metadata": {},
   "source": [
    "Here, we've downloaded a pretrained Sentence Transformer from the Hub and added a logistic classification head to the create the SetFit model. As indicated in the message, we need to train this model on some labeled examples. We can do so by using the `SetFitTrainer` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e44b7069-27b0-49ea-bc27-c44f94a98e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/c97k147160ddnklbfc2rk3hr0000gn/T/ipykernel_74208/2403420184.py:4: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.\n",
      "<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.\n",
      "/Users/maximekuil/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/mlflow/utils/requirements_utils.py:20: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources  # noqa: TID251\n",
      "/Users/maximekuil/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/huggingface_hub/utils/endpoint_helpers.py:163: FutureWarning: 'DatasetFilter' is deprecated and will be removed in huggingface_hub>=0.24. Please pass the filter parameters as keyword arguments directly to the `list_datasets` method.\n",
      "  warnings.warn(\n",
      "/Users/maximekuil/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/datasets/utils/_dill.py:379: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 4856.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitTrainer\n",
    "\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    num_iterations=20,\n",
    "    column_mapping={\"text\": \"text\", \"labels\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3e642",
   "metadata": {},
   "source": [
    "The main arguments to notice in the trainer is the following:\n",
    "\n",
    "* `loss_class`: The loss function to use for contrastive learning with the Sentence Transformer body\n",
    "* `num_iterations`: The number of text pairs to generate for contrastive learning\n",
    "* `column_mapping`: The `SetFitTrainer` expects the inputs to be found in a `text` and `label` column. This mapping automatically formats the training and evaluation datasets for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3c5ae-c287-4936-b1ac-5eca10c7f39c",
   "metadata": {},
   "source": [
    "Now that we've created a trainer, we can train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b5a468b-2796-47c3-8907-c0147ee58dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 2560\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 160\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/setfit/trainer.py:410\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, args, trial, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m train_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n\u001b[1;32m    406\u001b[0m full_parameters \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    407\u001b[0m     train_parameters \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;28;01melse\u001b[39;00m train_parameters\n\u001b[1;32m    408\u001b[0m )\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_classifier(\u001b[38;5;241m*\u001b[39mtrain_parameters, args\u001b[38;5;241m=\u001b[39margs)\n",
      "File \u001b[0;32m~/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/setfit/trainer.py:462\u001b[0m, in \u001b[0;36mTrainer.train_embeddings\u001b[0;34m(self, x_train, y_train, x_eval, y_eval, args)\u001b[0m\n\u001b[1;32m    459\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total optimization steps = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_train_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(total_train_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mwarmup_proportion)\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_sentence_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/setfit/trainer.py:643\u001b[0m, in \u001b[0;36mTrainer._train_sentence_transformer\u001b[0;34m(self, model_body, train_dataloader, eval_dataloader, args, loss_func, warmup_steps)\u001b[0m\n\u001b[1;32m    641\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_func(features, labels)\n\u001b[1;32m    642\u001b[0m     loss_value\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 643\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    646\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:20\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Simplon/fine-tune-classification/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:97\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     95\u001b[0m         clip_coef_clamped_device \u001b[38;5;241m=\u001b[39m clip_coef_clamped\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads:\n\u001b[0;32m---> 97\u001b[0m             \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799f994",
   "metadata": {},
   "source": [
    "The final step is to compute the model's performance using the `evaluate()` method. The default metric measures 'subset accuracy', which measures the fraction of samples where we predict all 8 labels correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c11d0-a1e4-49c2-859a-cc70e033b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021b168-02d2-4cc8-942d-65d3b821e253",
   "metadata": {},
   "source": [
    "And once the model is trained, you can push it to the Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420c4b9-1552-45a5-888c-cdbb78f8e4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainer.push_to_hub(f\"setfit-ethos-multilabel-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02173d18-4874-4148-8789-90ac695717bc",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `your-username/the-name-you-picked` so for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e5623-0452-4d36-8eb0-cfb36c8664da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "model = SetFitModel.from_pretrained(\"lewtun/setfit-ethos-multilabel-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0e4e5-6645-4306-a9ab-06d0db9e23d0",
   "metadata": {},
   "source": [
    "Run inference. As is usual in toxicity models, it tends to think any mention of topics such as race or gender are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51180c-5a62-4dfa-a543-334982db2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(\n",
    "    [\n",
    "        \"Jewish people often don't eat pork.\",\n",
    "        \"Is this lipstick suitable for people with dark skin?\"\n",
    "    ]\n",
    ")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc14708-c4f6-4137-b972-ac8c257bcb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predicted labels, requires you to have stored the 'features' somewhere\n",
    "[[f for f, p in zip(features, ps) if p] for ps in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24010af-dd8e-427c-b11d-e6f1d557d0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a53731e204626af339a5238c341a3f8c4bfd7cb5ccdda48ca3fe8366eef4175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
