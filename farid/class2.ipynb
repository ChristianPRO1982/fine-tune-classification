{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset.csv')\n",
    "\n",
    "#garder colonne description, cat1, cat2, cat3\n",
    "\n",
    "df = df[['description', 'cat1', 'cat2', 'cat3']]\n",
    "\n",
    "#supprimer les retours à la ligne dans la colonne description\n",
    "\n",
    "df['description'] = df['description'].str.replace('\\n', ' ')\n",
    "df.head(1)\n",
    "\n",
    "\n",
    "\n",
    "#supprimer colonne cat2 et cat3\n",
    "\n",
    "drop_columns = ['cat2', 'cat3']\n",
    "df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "#supprimer le manquant dans la colonne description\n",
    "\n",
    "df = df.dropna(subset=['description'])\n",
    "\n",
    "#Labeencoder cat1\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['cat1_encoded'] = label_encoder.fit_transform(df['cat1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_description, test_description , train_label, test_label = train_test_split(df['description'], df['cat1_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir le nombre de classes\n",
    "num_classes = len(df['cat1_encoded'].unique())\n",
    "\n",
    "#initialise le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\", num_labels=num_classes)\n",
    "\n",
    "#tokeniser les données\n",
    "train_encodings = tokenizer(list(train_description), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_description), truncation=True, padding=True)\n",
    "\n",
    "#convertir les labels en tensor\n",
    "train_labels = torch.tensor(list(train_label))\n",
    "test_labels = torch.tensor(list(test_label))\n",
    "\n",
    "#convertir les encodings en tensor\n",
    "train_input_ids = torch.tensor(train_encodings.input_ids)\n",
    "train_attention_mask = torch.tensor(train_encodings.attention_mask)\n",
    "test_input_ids = torch.tensor(test_encodings.input_ids)\n",
    "test_attention_mask = torch.tensor(test_encodings.attention_mask)\n",
    "\n",
    "#créer dataloader\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "#initialiser l'optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fonction d'entraînement\n",
    "def train(model, dataloader, optimizer, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# entraîner le modèle\n",
    "train(model, train_loader, optimizer, epochs=1)\n",
    "\n",
    "#tester le modèle\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {correct/total}')\n",
    "\n",
    "test(model, test_loader)\n",
    "\n",
    "#sauvegarder le modèle\n",
    "model.save_pretrained('model')\n",
    "tokenizer.save_pretrained('model')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
