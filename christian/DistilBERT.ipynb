{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUGGINGFACE + TENSORFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### PREPARATION\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model pré-traité\n",
    "# huggingface_model = \"distilbert/distilbert-base-uncased\"\n",
    "huggingface_model = \"almanach/camembert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>cat</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>class</th>\n",
       "      <th>class_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.tourisme-cambresis.fr/1-les-templi...</td>\n",
       "      <td>Aventure-jeu : \"Les Templiers du coffre d'or\"</td>\n",
       "      <td>Le jeu aventure « Les templiers du coffre d’or...</td>\n",
       "      <td>17</td>\n",
       "      <td>Jeu</td>\n",
       "      <td>Famille</td>\n",
       "      <td>Détente</td>\n",
       "      <td>Action</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.tourisme-cambresis.fr/1-les-templi...   \n",
       "\n",
       "                                           title  \\\n",
       "0  Aventure-jeu : \"Les Templiers du coffre d'or\"   \n",
       "\n",
       "                                         description  cat cat1     cat2  \\\n",
       "0  Le jeu aventure « Les templiers du coffre d’or...   17  Jeu  Famille   \n",
       "\n",
       "      cat3   class  class_number  \n",
       "0  Détente  Action           0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset.csv')\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb ligne df 391\n",
      "nb ligne df1 390\n",
      "nb ligne df2 317\n",
      "nb ligne df3 134\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.tourisme-cambresis.fr</td>\n",
       "      <td>https://www.tourisme-cambresis.fr/1-les-templi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain  \\\n",
       "0  www.tourisme-cambresis.fr   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.tourisme-cambresis.fr/1-les-templi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_domain(url):\n",
    "    url = url[url.find('//')+2:]\n",
    "    url = url[:url.find('/')]\n",
    "    return url\n",
    "\n",
    "df['domain'] = df['url'].apply(extract_domain)\n",
    "\n",
    "print(\"nb ligne df\", len(df))\n",
    "\n",
    "df = df.dropna(subset=['description'])\n",
    "\n",
    "print(\"nb ligne df1\", len(df))\n",
    "\n",
    "df1 = df.copy()\n",
    "df2 = df.dropna(subset=['cat2'])\n",
    "df3 = df.dropna(subset=['cat3'])\n",
    "\n",
    "print(\"nb ligne df2\", len(df2))\n",
    "print(\"nb ligne df3\", len(df3))\n",
    "\n",
    "df[['domain', 'url']].head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.tourisme-cambresis.fr | Aventure-jeu : \"Le...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  www.tourisme-cambresis.fr | Aventure-jeu : \"Le...     17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concaténer les colonnes 'title' et 'description' pour former les textes\n",
    "df1['text'] = df1['domain'] + \" | \" + df1['title'] + \" \" + df1['description']\n",
    "# df1['text'] = df1['title']\n",
    "df1['label'] = df1['cat']\n",
    "\n",
    "df_ml = df1[['text', 'label']]\n",
    "df_ml.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/Documents/projects/DEV_IA/fine-tune-classification/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "df_train = df_ml.iloc[:190]\n",
    "df_test = df_ml.iloc[190:380]\n",
    "df_unsupervised = df_ml.iloc[380:]\n",
    "\n",
    "# Conversion des DataFrames en Datasets\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "dataset_unsupervised = Dataset.from_pandas(df_unsupervised)\n",
    "\n",
    "# Créer un DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'test': dataset_test,\n",
    "    'unsupervised': dataset_unsupervised\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# imdb = load_dataset('imdb')\n",
    "imdb = dataset_dict\n",
    "imdb = imdb.remove_columns([\"__index_level_0__\"])\n",
    "imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'www.tourismevalenciennes.fr | Exposition \"Quand la nature inspire l\\'innovation\" - Famars Va prendre tes leçons dans la nature, c’est là qu’est notre futur.\\nLéonard de Vinci\\n\\n\\nCe temps fort exceptionnel, préfiguration à la grande exposition Biomimétisme prévue en septembre 2024, réunira durant 3 semaines une exposition, des ateliers, une conférence…\\n\\nLe biomimétisme, littéralement, l’imitation du vivant, consiste à s’inspirer de la nature pour concevoir de nouvelles technologies, innover. C’est, par exemple, en s’inspirant des oiseaux et de leurs ailes que sont nés les premiers ancêtres des avions. Que de chemin parcouru depuis, dans tous les domaines !\\n\\nL’EXPOSITION*, du 2 au 20 octobre, accessible dès 10 ans, sera certainement une révélation pour beaucoup de visiteurs !\\nÀ travers quelques exemples concrets, on découvre comment le vivant a été, est et sera une source d’inspiration exceptionnelle pour l’innovation. L’Aviation, le ferroviaire, le sport, l’automobile sont autant de secteurs industriels majeurs pour qui l’observation de la nature demeure un travail fondamental. À partir de ces observations, des avancées majeures ont pu avoir lieu.\\n\\nAujourd’hui, le biomimétisme peut apporter des solutions aux grands enjeux de développement durable auxquels nous sommes tous confrontés.\\nLa biodiversité n’est pas uniquement un espace fragile à protéger, c’est aussi une bibliothèque de technologies et une incroyable source d’inspirations !\\n\\n*Retrouvez les différents tarifs sur le site Internet*',\n",
       " 'label': 11}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/Documents/projects/DEV_IA/fine-tune-classification/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# générateur de tokens\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de prétraitement des tokens pour les tronqués pour par qu'ils dépassent la longueur max d'entrée du modèle\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 190/190 [00:00<00:00, 3643.70 examples/s]\n",
      "Map: 100%|██████████| 190/190 [00:00<00:00, 4221.95 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1501.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# application de la fonction avec un accélérateur de mapping\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like, you can create a smaller subset of the full dataset to fine-tune on to reduce the time it takes\n",
    "small_train_dataset = tokenized_imdb[\"train\"].shuffle(seed=42).select(range(190))\n",
    "small_eval_dataset = tokenized_imdb[\"test\"].shuffle(seed=42).select(range(190))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 12:11:22.301949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 12:11:22.436910: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:22.436929: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-06-10 12:11:23.176724: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:23.176845: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:23.176857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Now create a batch of examples using DataCollatorWithPadding. It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# TS\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### HYPERPARAMETRES\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label: {0: 'Action', 1: 'Art', 2: 'Atelier', 3: 'Balade', 4: 'Brocante', 5: 'Concert', 6: 'Conférence', 7: 'Culture', 8: 'Danse', 9: 'Détente', 10: 'Environnement', 11: 'Exposition', 12: 'Famille', 13: 'Festival', 14: 'Fête', 15: 'Gastronomie', 16: 'Histoire', 17: 'Jeu', 18: 'Marché', 19: 'Santé', 20: 'Spectacle', 21: 'Sport', 22: 'Théatre', 23: 'Visite'}\n",
      "label2id: {'Action': 0, 'Art': 1, 'Atelier': 2, 'Balade': 3, 'Brocante': 4, 'Concert': 5, 'Conférence': 6, 'Culture': 7, 'Danse': 8, 'Détente': 9, 'Environnement': 10, 'Exposition': 11, 'Famille': 12, 'Festival': 13, 'Fête': 14, 'Gastronomie': 15, 'Histoire': 16, 'Jeu': 17, 'Marché': 18, 'Santé': 19, 'Spectacle': 20, 'Sport': 21, 'Théatre': 22, 'Visite': 23}\n"
     ]
    }
   ],
   "source": [
    "# création de IDs pour les labels\n",
    "# id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "# label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "id2label[0] = 'Action'\n",
    "id2label[1] = 'Art'\n",
    "id2label[2] = 'Atelier'\n",
    "id2label[3] = 'Balade'\n",
    "id2label[4] = 'Brocante'\n",
    "id2label[5] = 'Concert'\n",
    "id2label[6] = 'Conférence'\n",
    "id2label[7] = 'Culture'\n",
    "id2label[8] = 'Danse'\n",
    "id2label[9] = 'Détente'\n",
    "id2label[10] = 'Environnement'\n",
    "id2label[11] = 'Exposition'\n",
    "id2label[12] = 'Famille'\n",
    "id2label[13] = 'Festival'\n",
    "id2label[14] = 'Fête'\n",
    "id2label[15] = 'Gastronomie'\n",
    "id2label[16] = 'Histoire'\n",
    "id2label[17] = 'Jeu'\n",
    "id2label[18] = 'Marché'\n",
    "id2label[19] = 'Santé'\n",
    "id2label[20] = 'Spectacle'\n",
    "id2label[21] = 'Sport'\n",
    "id2label[22] = 'Théatre'\n",
    "id2label[23] = 'Visite'\n",
    "\n",
    "\n",
    "label2id['Action'] = 0\n",
    "label2id['Art'] = 1\n",
    "label2id['Atelier'] = 2\n",
    "label2id['Balade'] = 3\n",
    "label2id['Brocante'] = 4\n",
    "label2id['Concert'] = 5\n",
    "label2id['Conférence'] = 6\n",
    "label2id['Culture'] = 7\n",
    "label2id['Danse'] = 8\n",
    "label2id['Détente'] = 9\n",
    "label2id['Environnement'] = 10\n",
    "label2id['Exposition'] = 11\n",
    "label2id['Famille'] = 12\n",
    "label2id['Festival'] = 13\n",
    "label2id['Fête'] = 14\n",
    "label2id['Gastronomie'] = 15\n",
    "label2id['Histoire'] = 16\n",
    "label2id['Jeu'] = 17\n",
    "label2id['Marché'] = 18\n",
    "label2id['Santé'] = 19\n",
    "label2id['Spectacle'] = 20\n",
    "label2id['Sport'] = 21\n",
    "label2id['Théatre'] = 22\n",
    "label2id['Visite'] = 23\n",
    "\n",
    "print(\"id2label:\", id2label)\n",
    "print(\"label2id:\", label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entraienement avec DistilBERT\n",
    "# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"my_awesome_model\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=False,\n",
    "#     no_cuda=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### EVALUATION\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  avec fonction évaluer les prédictions\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# métrics\n",
    "# import numpy as np\n",
    "# import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitoring\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### ENTRAINEMENT\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=small_eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=data_collator,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assurez-vous que le GPU est désactivé dans torch également\n",
    "# import torch\n",
    "# torch.cuda.is_available = lambda: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# Tensorflow\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 12:11:26.809094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-10 12:11:26.809588: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:26.809703: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:26.809796: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:26.809883: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:26.809991: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:26.810078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-06-10 12:11:26.810136: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-06-10 12:11:26.811698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# TS\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/Documents/projects/DEV_IA/fine-tune-classification/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFCamembertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    # \"distilbert/distilbert-base-uncased\",\n",
    "    huggingface_model,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/utilisateur/Documents/projects/DEV_IA/fine-tune-classification/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_imdb[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_imdb[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)  # No loss argument!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/utilisateur/.cache/huggingface/token\n",
      "Login successful\n",
      "\n",
      "<> login huggingface <>\n",
      "PlumePJ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login('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')\n",
    "print()\n",
    "print('<> login huggingface <>')\n",
    "os.system('huggingface-cli whoami')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/Documents/projects/DEV_IA/fine-tune-classification/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/PlumePJ/my_awesome_model into local empty directory.\n",
      "Download file tf_model.h5:   0%|          | 52.0k/422M [00:01<2:43:59, 45.0kB/s]\n",
      "Download file tf_model.h5:  99%|█████████▉| 418M/422M [00:46<00:00, 9.54MB/s]   \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Download file tf_model.h5: 100%|██████████| 422M/422M [00:50<00:00, 8.83MB/s]\n",
      "Download file sentencepiece.bpe.model: 100%|██████████| 792k/792k [00:49<00:00, 16.3kB/s]\n",
      "\n",
      "\u001b[A\n",
      "Clean file sentencepiece.bpe.model: 100%|██████████| 792k/792k [00:49<?, ?B/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Clean file tf_model.h5: 100%|██████████| 422M/422M [00:03<00:00, 136MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [metric_callback, push_to_hub_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "47/47 [==============================] - 382s 8s/step - loss: 3.0945 - val_loss: 3.0510 - accuracy: 0.2105\n",
      "Epoch 2/3\n",
      "47/47 [==============================] - 363s 8s/step - loss: 3.0305 - val_loss: 3.0486 - accuracy: 0.2105\n",
      "Epoch 3/3\n",
      "47/47 [==============================] - 341s 7s/step - loss: 3.0303 - val_loss: 3.0486 - accuracy: 0.2105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fa83c9f00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
